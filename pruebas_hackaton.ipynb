{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNeRi2xWAZRpyQbkCxNkgQk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gordenbuh/ClimaMetrics/blob/main/pruebas_hackaton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- (Código anterior: Instalación de librerías, montaje de Drive, definición de rutas,\n",
        "# ---  carga/simulación de datos, preprocesamiento, entrenamiento del modelo de ML,\n",
        "# ---  carga del modelo de Hugging Face, función generar_informe_alerta_hf) ---\n",
        "\n",
        "# --- (Reutilizamos el código anterior hasta la parte de integración con el LLM) ---\n",
        "\n",
        "# --- Preparación del Entorno en Google Colab ---\n",
        "\n",
        "# 1. Instalación de librerías (¡importante en Colab!)\n",
        "!pip install pandas numpy scikit-learn matplotlib seaborn transformers torch\n",
        "\n",
        "# 2. Montar Google Drive (opcional, pero MUY recomendado)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 3. Definir rutas (ajustar a tu estructura de carpetas en Drive)\n",
        "#    Si no usas Google Drive, comenta estas líneas y ajusta las rutas a archivos locales\n",
        "project_path = '/content/drive/MyDrive/MiProyectoHackathon/'  # Cambiar a tu ruta\n",
        "data_path = os.path.join(project_path, 'datos/')\n",
        "model_path = os.path.join(project_path, 'modelos/')\n",
        "\n",
        "# Crea los directorios si no existen\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, mean_squared_error, r2_score, confusion_matrix, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n"
      ],
      "metadata": {
        "id": "z1BGRR4nLDwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- 1. RECOPILACIÓN Y PREPARACIÓN DE DATOS ---\n",
        "\n",
        "# Simulación de datos (reemplazar con tus datos reales)\n",
        "# Si tienes datos reales, cárgalos desde Drive (o desde donde los tengas)\n",
        "# Ejemplo (si tienes un CSV en Drive):\n",
        "# df = pd.read_csv(os.path.join(data_path, 'tus_datos.csv'))\n",
        "\n",
        "np.random.seed(42)\n",
        "num_samples = 1000\n",
        "data = {\n",
        "    'precipitacion_24h': np.random.rand(num_samples) * 200,\n",
        "    'precipitacion_72h': np.random.rand(num_samples) * 300,\n",
        "    'nivel_rio': np.random.rand(num_samples) * 5 + np.random.rand(num_samples) * 2 * (np.random.rand(num_samples) > 0.7),\n",
        "    'humedad_suelo': np.random.rand(num_samples) * 0.6 + 0.4 * (np.random.rand(num_samples) > 0.8),\n",
        "    'temperatura': np.random.rand(num_samples) * 20 + 10,\n",
        "    'inundacion': np.zeros(num_samples)\n",
        "}\n",
        "for i in range(num_samples):\n",
        "    if (data['precipitacion_24h'][i] > 100 and data['nivel_rio'][i] > 4) or \\\n",
        "       (data['precipitacion_72h'][i] > 200 and data['humedad_suelo'][i] > 0.8) or \\\n",
        "       (data['nivel_rio'][i] > 6):\n",
        "        data['inundacion'][i] = 1\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "sWzjVyUQMzT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(df.head())\n",
        "print(df.describe())\n",
        "print(df['inundacion'].value_counts())  # Verificar el balance de clases\n"
      ],
      "metadata": {
        "id": "ypqUCVTrM4Pb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Visualización de datos para ver la proporcion de 1/0\n",
        "sns.pairplot(df, hue='inundacion', diag_kind='kde')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_JIoXgQFM_sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definición de las variables de desarrollo\n",
        "\n",
        "X = df.drop('inundacion', axis=1)\n",
        "y = df['inundacion']\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_val_scaled = scaler.transform(X_val)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "\n",
        "# --- 2. DESARROLLO DEL MODELO DE ML ---\n",
        "\n",
        "# Modelo de ML (ejemplo con Random Forest, pero puedes usar otros)\n",
        "model = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)  # Ajusta hiperparámetros\n",
        "# n_estimators: número de árboles.  Más árboles = mejor, pero más lento.\n",
        "# max_depth: profundidad máxima de los árboles.  Evita overfitting.\n",
        "\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# --- Guardar el modelo de ML (en Google Drive, si lo has montado) ---\n",
        "import joblib  # Para guardar modelos de scikit-learn\n",
        "model_filename = os.path.join(model_path, 'modelo_ml.joblib')  # Ruta completa\n",
        "joblib.dump(model, model_filename)\n",
        "print(f\"Modelo de ML guardado en: {model_filename}\")\n",
        "\n",
        "# --- (Si quieres cargar el modelo más tarde) ---\n",
        "# model = joblib.load(model_filename)\n",
        "\n"
      ],
      "metadata": {
        "id": "Px8YGWLVNDNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- 3. INTEGRACIÓN CON EL LLM (Hugging Face) ---\n",
        "\n",
        "# --- Configuración del LLM ---\n",
        "\n",
        "# Elige tu modelo\n",
        "model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"  # O el que prefieras\n",
        "\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "    model_llm = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",  # Usa GPU si está disponible\n",
        "        torch_dtype=torch.float16,  # Usa float16 para ahorrar memoria (si es compatible)\n",
        "        # low_cpu_mem_usage=True  # Reduce el uso de CPU RAM (útil en Colab)\n",
        "    )\n",
        "\n",
        "    pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model_llm,\n",
        "        tokenizer=tokenizer,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=True,\n",
        "        temperature=0.7,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        repetition_penalty=1.2,\n",
        "        num_return_sequences=1,\n",
        "        # batch_size=8,  # Ajusta el tamaño del batch si tienes problemas de memoria (¡importante en Colab!)\n",
        "    )\n",
        "\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar el modelo '{model_name}' de Hugging Face: {e}\")\n",
        "    raise\n"
      ],
      "metadata": {
        "id": "Dp4rHZvhNXcz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_informe_alerta_hf(variables_criticas, prediccion, ubicacion=\"una zona de prueba\"):\n",
        "    prompt = f\"\"\"\n",
        "Contexto: Estamos en {ubicacion}. El modelo de predicción de inundaciones ha identificado las siguientes\n",
        "variables como las más influyentes:\n",
        "\"\"\"\n",
        "    for var, valor in variables_criticas.items():\n",
        "        prompt += f\"- {var}: {valor}\\n\"\n",
        "    prompt += f\"El modelo predice una probabilidad de inundación de {prediccion:.2f}.\\n\\n\"\n",
        "    prompt += \"\"\"\n",
        "Instrucción: Genera un informe de alerta de inundación conciso y fácil de entender para el público en general. Incluye:\n",
        "- Nivel de alerta (bajo, medio, alto).\n",
        "- Áreas potencialmente afectadas (se específico en la medida de lo posible basándote en la ubicación y el nivel de riesgo, pero no inventes nombres si no los tienes).\n",
        "- Recomendaciones de seguridad (evacuación, preparación, etc.).  Sé lo más específico posible, adaptando las recomendaciones al nivel de alerta.\n",
        "- Hora y fecha de emisión de la alerta.\n",
        "- Un llamado a la acción. (Por ejemplo: \"Manténgase informado a través de los canales oficiales\").\n",
        "Formato: Usa viñetas para cada sección del informe.\n",
        "\"\"\"\n",
        "    try:\n",
        "      #Para modelos que usan un formato de chat (ej: Mistral)\n",
        "      if \"mistralai\" in model_name.lower() or \"llama\" in model_name.lower():\n",
        "        formatted_prompt = f\"<s>[INST] {prompt} [/INST]\"\n",
        "        result = pipe(formatted_prompt)\n",
        "        informe = result[0]['generated_text'].replace(formatted_prompt, \"\").strip()\n",
        "      # Si usas un modelo tipo T5 (como Flan-T5)\n",
        "      elif \"t5\" in model_name.lower(): #Para modelos que usan una estructura Input/Output como T5\n",
        "          input_prompt = f\"Genera informe de alerta: {prompt}\"\n",
        "          result = pipe(input_prompt)\n",
        "          informe = result[0]['generated_text'].strip()\n",
        "\n",
        "      else: #En caso de usar otro modelo, se puede intentar una aproximación\n",
        "          result = pipe(prompt)\n",
        "          informe = result[0]['generated_text'].strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error al generar el informe con el modelo de Hugging Face: {e}\")\n",
        "        informe = \"No se pudo generar el informe debido a un error.\"\n",
        "\n",
        "    return informe\n"
      ],
      "metadata": {
        "id": "QIyNMx2mNdAm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# --- INFORME PARA EL REGISTRO CON MAYOR PROBABILIDAD ---\n",
        "\n",
        "# 1. Predecir probabilidades para todos los registros de prueba\n",
        "probabilidades = model.predict_proba(X_test_scaled)[:, 1]  # Probabilidad de la clase 1 (inundación)\n",
        "\n",
        "# 2. Encontrar el índice del registro con la probabilidad más alta\n",
        "indice_max_prob = np.argmax(probabilidades)\n",
        "\n",
        "# 3. Obtener los datos de ESE registro\n",
        "registro_max_prob = X_test.iloc[indice_max_prob]\n",
        "probabilidad_max = probabilidades[indice_max_prob]\n",
        "\n",
        "\n",
        "# 4. Obtener las variables más importantes para el informe (usando el modelo de ML)\n",
        "if hasattr(model, 'feature_importances_'):\n",
        "    importances = model.feature_importances_\n",
        "    feature_names = X.columns\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    top_variables = [feature_names[i] for i in indices[:3]]  # Top N variables\n",
        "    variables_criticas = {var: registro_max_prob[var] for var in top_variables}\n",
        "elif hasattr(model, 'coef_'):\n",
        "    importances = np.abs(model.coef_[0])\n",
        "    feature_names = X.columns\n",
        "    indices = np.argsort(importances)[::-1]\n",
        "    top_variables = [feature_names[i] for i in indices[:3]]\n",
        "    variables_criticas = {var: registro_max_prob[var] for var in top_variables}\n",
        "else:  # Si el modelo no proporciona importancia de variables\n",
        "    variables_criticas = {\n",
        "        'precipitacion_24h': registro_max_prob['precipitacion_24h'],\n",
        "        'nivel_rio': registro_max_prob['nivel_rio'],\n",
        "        'humedad_suelo': registro_max_prob['humedad_suelo']\n",
        "    }\n",
        "    top_variables = list(variables_criticas.keys())\n",
        "\n",
        "\n",
        "# 5. Generar el informe para el registro con mayor riesgo\n",
        "informe_especifico = generar_informe_alerta_hf(\n",
        "    variables_criticas, probabilidad_max, ubicacion=\"la zona de pruebas\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- INFORME DE ALERTA (Registro con Mayor Probabilidad) ---\")\n",
        "print(informe_especifico)\n",
        "\n"
      ],
      "metadata": {
        "id": "u7ReHn4WNo1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# --- (Opcional) Sistema de Alerta (simulación) ---\n",
        "# (El código del sistema de alerta es el mismo, pero ahora podrías usar\n",
        "#  informe_especifico en lugar de un informe genérico)\n",
        "def enviar_alerta(informe, destinatarios):\n",
        "    print(f\"\\n--- Enviando Alerta ---\")\n",
        "    print(f\"Destinatarios: {', '.join(destinatarios)}\")\n",
        "    print(f\"Mensaje:\\n{informe}\")\n",
        "    print(\"Alerta enviada (simulación).\")\n",
        "destinatarios_prueba = [\"+1234567890\", \"usuario1@example.com\", \"usuario2@example.com\"]\n",
        "enviar_alerta(informe_especifico, destinatarios_prueba) #Usamos el informe especifico."
      ],
      "metadata": {
        "id": "a6awL-j2N3Rs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}